{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\przem\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad.\n",
    "Wczytaj dane i narysuj je na wykresie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_str = '''Region Alcohol Tobacco\n",
    "North 6.47 4.03\n",
    "Yorkshire 6.13 3.76\n",
    "Northeast 6.19 3.77\n",
    "East_Midlands 4.89 3.34\n",
    "West_Midlands 5.63 3.47\n",
    "East_Anglia 4.52 2.92\n",
    "Southeast 5.89 3.20\n",
    "Southwest 4.79 2.71\n",
    "Wales 5.27 3.53\n",
    "Scotland 6.08 4.51\n",
    "Northern_Ireland 4.02 4.56'''\n",
    "from io import StringIO\n",
    "df = pd.read_csv(StringIO(data_str), sep=r'\\s+')\n",
    "\n",
    "X_train =  np.vstack(df.Tobacco)\n",
    "y_train = np.vstack(df.Alcohol)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_train, y_train, \"bo\")\n",
    "plt.show()\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad \n",
    "Wykonaj regresię za pomocą Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.35116849]\n",
      " [0.30193836]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_lr = lambda x: lin_reg.coef_[0][0] * x +lin_reg.intercept_[0]\n",
    "\n",
    "x_f_lr = np.linspace(2.5, 5, 200)\n",
    "y_f_lr = f_lr(x_f_lr)\n",
    "plt.plot(x_f_lr, y_f_lr, 'g');\n",
    "plt.plot(X_train, y_train, 'bo');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad.\n",
    "Proszę narysować wykres zmiany funkcji kosztu w przypadku:\n",
    "  * Gradient Descent\n",
    "  * Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 2\n"
     ]
    }
   ],
   "source": [
    "data_plus_bias  = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "data_m, data_n = data_plus_bias.shape\n",
    "print(data_m, data_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 5.90421\n",
      "Epoch 100 MSE = 0.57923687\n",
      "Epoch 200 MSE = 0.7844822\n",
      "Epoch 300 MSE = 0.6125314\n",
      "Epoch 400 MSE = 0.5581452\n",
      "Epoch 500 MSE = 0.6469549\n",
      "Epoch 600 MSE = 0.6220324\n",
      "Epoch 700 MSE = 0.56872416\n",
      "Epoch 800 MSE = 0.746707\n",
      "Epoch 900 MSE = 0.5702867\n",
      "Best theta:\n",
      "[[2.283169 ]\n",
      " [0.8564258]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "error_array_g_b=[]\n",
    "error_array_m_b=[]\n",
    "\n",
    "batch_error_array_g=[]\n",
    "batch_error_array_m=[]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, data_n), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([data_n, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "n_batches = int(np.ceil(data_m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size, m):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = data_plus_bias[indices] \n",
    "    y_batch = y_train[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size, data_m)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )   \n",
    "        \n",
    "        batch_error_array_g.append( sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )\n",
    "        error_array_g_b.append( sess.run( mse, feed_dict={X: data_plus_bias, y: y_train}) )            \n",
    "    \n",
    "    best_theta_g = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final_g.ckpt\")\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 5.90421\n",
      "Epoch 100 MSE = 0.5684159\n",
      "Epoch 200 MSE = 0.6026674\n",
      "Epoch 300 MSE = 0.5266076\n",
      "Epoch 400 MSE = 0.46494687\n",
      "Epoch 500 MSE = 0.49987543\n",
      "Epoch 600 MSE = 0.49797657\n",
      "Epoch 700 MSE = 0.4399765\n",
      "Epoch 800 MSE = 0.664523\n",
      "Epoch 900 MSE = 0.45663422\n",
      "Best theta:\n",
      "[[4.317688  ]\n",
      " [0.22942916]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, data_n), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([data_n, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "n_batches = int(np.ceil(data_m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size, m):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = data_plus_bias[indices] \n",
    "    y_batch = y_train[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size, data_m)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )   \n",
    "        \n",
    "        batch_error_array_m.append( sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )\n",
    "        error_array_m_b.append( sess.run( mse, feed_dict={X: data_plus_bias, y: y_train}) )            \n",
    "    \n",
    "    best_theta_m = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final_m.ckpt\")\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(error_array_g_b, label='Batch Gradient Descent')\n",
    "plt.semilogy(error_array_m_b, label='Batch Momentum')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(batch_error_array_g, label='Gradient Descent')\n",
    "plt.semilogy(batch_error_array_m, label='Momentum')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Batch MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lr_g = lambda x: best_theta_g[1] * x + best_theta_g[0]\n",
    "f_lr_m = lambda x: best_theta_m[1] * x + best_theta_m[0]\n",
    "\n",
    "x_f_lr = np.linspace(2.5, 5, 200)\n",
    "y_f_lr_g = f_lr_g(x_f_lr)\n",
    "y_f_lr_m = f_lr_m(x_f_lr)\n",
    "plt.plot(x_f_lr, y_f_lr_g, 'g');\n",
    "plt.plot(x_f_lr, y_f_lr_m, 'r');\n",
    "plt.plot(X_train, y_train, 'bo');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Proszę zmienić funkcję kosztu na\n",
    "$$\n",
    "MSE(X;\\theta) = \\frac{1}{m} \\sum | \\theta^T x_i - y_i  |.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "error_array_g_b=[]\n",
    "error_array_m_b=[]\n",
    "\n",
    "batch_error_array_g=[]\n",
    "batch_error_array_m=[]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, data_n), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([data_n, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.abs(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "n_batches = int(np.ceil(data_m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size, m):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = data_plus_bias[indices] \n",
    "    y_batch = y_train[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size, data_m)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )   \n",
    "        \n",
    "        batch_error_array_g.append( sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )\n",
    "        error_array_g_b.append( sess.run( mse, feed_dict={X: data_plus_bias, y: y_train}) )            \n",
    "    \n",
    "    best_theta_g = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final_g.ckpt\")\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, data_n), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([data_n, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.abs(error), name=\"mse\")\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "n_batches = int(np.ceil(data_m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size, m):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = data_plus_bias[indices] \n",
    "    y_batch = y_train[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size, data_m)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )   \n",
    "        \n",
    "        batch_error_array_m.append( sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )\n",
    "        error_array_m_b.append( sess.run( mse, feed_dict={X: data_plus_bias, y: y_train}) )            \n",
    "    \n",
    "    best_theta_m = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final_m.ckpt\")\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(error_array_g_b, label='Batch Gradient Descent')\n",
    "plt.semilogy(error_array_m_b, label='Batch Momentum')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(batch_error_array_g, label='Gradient Descent')\n",
    "plt.semilogy(batch_error_array_m, label='Momentum')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Batch MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lr_g = lambda x: best_theta_g[1] * x + best_theta_g[0]\n",
    "f_lr_m = lambda x: best_theta_m[1] * x + best_theta_m[0]\n",
    "\n",
    "x_f_lr = np.linspace(2.5, 5, 200)\n",
    "y_f_lr_g = f_lr_g(x_f_lr)\n",
    "y_f_lr_m = f_lr_m(x_f_lr)\n",
    "plt.plot(x_f_lr, y_f_lr_g, 'g');\n",
    "plt.plot(x_f_lr, y_f_lr_m, 'r');\n",
    "plt.plot(X_train, y_train, 'bo');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
