{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Scopes\n",
    "\n",
    "W przypadku bardziej złożonych modeli, takich jak sieci neuronowe, wykres może być skomplikowany (mamy tysiące węzłów). Aby tego uniknąć, możesz utworzyć zakresy nazw, aby grupować powiązane węzły. Na przykład zmodyfikujmy poprzedni kod, aby zdefiniować **error** i **mse** w name scopes **loss**:\n",
    "\n",
    "```python\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad.\n",
    "Wczytaj dane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zawsze unormuj dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "housing_data_scaled=scaler.fit_transform(housing.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train =  housing_data_scaled\n",
    "y_train = housing.target.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad.\n",
    "  * Proszę narysować wykres zmiany funkcji kosztu w przypadku:\n",
    "    * Gradient Descent\n",
    "    * Momentum\n",
    "  dla 100 iteracji.\n",
    "\n",
    "  * Następnie zapisać model.\n",
    "\n",
    "  * I wykonać następne 100 iteracji po ponownym wczytaniu.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640 9\n"
     ]
    }
   ],
   "source": [
    "data_plus_bias  = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "data_m, data_n = data_plus_bias.shape\n",
    "print(data_m, data_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 0.31841922\n",
      "Epoch 10 MSE = 0.8001851\n",
      "Epoch 20 MSE = 0.5560062\n",
      "Epoch 30 MSE = 0.44753096\n",
      "Epoch 40 MSE = 0.54900646\n",
      "Epoch 50 MSE = 0.80266595\n",
      "Epoch 60 MSE = 0.55392677\n",
      "Epoch 70 MSE = 0.43071902\n",
      "Epoch 80 MSE = 0.27837253\n",
      "Epoch 90 MSE = 0.7054215\n",
      "Best theta:\n",
      "[[ 2.0728784 ]\n",
      " [ 0.8359189 ]\n",
      " [ 0.11120823]\n",
      " [-0.25528795]\n",
      " [ 0.29422897]\n",
      " [-0.00615538]\n",
      " [-0.04408351]\n",
      " [-0.9086386 ]\n",
      " [-0.8651398 ]]\n"
     ]
    }
   ],
   "source": [
    "logs_path = \"./tf_logs/run_number_1_solution/\"\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "\n",
    "error_array_g_b=[]\n",
    "error_array_m_b=[]\n",
    "\n",
    "batch_error_array_g=[]\n",
    "batch_error_array_m=[]\n",
    "\n",
    "\n",
    "with tf.name_scope(\"reader\"):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, data_n), name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "    theta = tf.Variable(tf.random_uniform([data_n, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "    y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "    \n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    \n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "with tf.name_scope(\"train\"):    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(mse)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"mse\", mse)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "n_batches = int(np.ceil(data_m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size, m):\n",
    "    np.random.seed(epoch * n_batches + batch_index)  \n",
    "    indices = np.random.randint(m, size=batch_size)  \n",
    "    X_batch = data_plus_bias[indices] \n",
    "    y_batch = y_train[indices] \n",
    "    return X_batch, y_batch\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    ###############################    \n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "    ###############################\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size, data_m)\n",
    "            #sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            ############################### \n",
    "            _, summary = sess.run( [training_op, merged_summary_op] , feed_dict={X: X_batch, y: y_batch})\n",
    "            summary_writer.add_summary(summary, epoch * n_batches + batch_index)\n",
    "            ###############################\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )   \n",
    "        \n",
    "        batch_error_array_g.append( sess.run( mse, feed_dict={X: X_batch, y: y_batch}) )\n",
    "        error_array_g_b.append( sess.run( mse, feed_dict={X: data_plus_bias, y: y_train}) )            \n",
    "    \n",
    "    best_theta_g = theta.eval()\n",
    "    save_path = saver.save(sess, \"./regression/my_model_final_g.ckpt\")\n",
    "    \n",
    "print(\"Best theta:\")\n",
    "print(best_theta_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Uruchom w konsoli\n",
    "```python\n",
    "tensorboard --logdir ./tf_logs\n",
    "\n",
    "tensorboard --logdir ./tf_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
