{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP w TensorFlow\n",
    "\n",
    "Najpierw musimy zaimportować bibliotekę tensorflow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie musimy określić liczbę wejść i wyjść oraz ustawić liczbę ukrytych neuronów w każdej warstwie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 28*28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie, można użyć węzłów typu placeholder do reprezentowania danych treningowych. \n",
    "\n",
    "  * Kształt $X$ jest tylko częściowo zdefiniowany. \n",
    "  * Wiemy, że będzie to tensor 2D, z instancjami wzdłuż pierwszego wymiaru i cechami wzdłuż drugiego wymiaru, i wiemy, że liczba funkcji będzie wynosić $28 x 28$ (jedna cecha na piksel), ale nie wiemy jeszcze, ile wystąpi elementów. \n",
    "  * Zatem kształt $X$ to (None, n_inputs)\n",
    "  * Podobnie, wiemy, że $y$ będzie tensorem 1D z jedą kolumną na instancję, ale znowu nie znamy wielkości zbioru treningowego, więc kształt jest (None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teraz stwórzmy właściwą sieć neuronową. \n",
    "\n",
    "  * Placeholder $X$ będzie działał jako warstwa wejściowa; \n",
    "  * w fazie wykonania do placecholdera zastanie podana jedna część (batch) zbioru treningowego (należy pamiętać, że wszystkie elementy batcha będą przetwarzane jednocześnie przez sieć neuronową). \n",
    "\n",
    "Teraz musisz utworzyć dwie ukryte warstwy i warstwę wyjściową. Dwie ukryte warstwy są prawie identyczne: \n",
    "\n",
    "  * różnią się tylko wejściami, z którymi są połączone, \n",
    "  * oraz liczbą neuronów, które zawierają. \n",
    "\n",
    "Warstwa wyjściowa jest również bardzo podobna, ale wykorzystuje funkcję aktywacji softmax zamiast funkcji aktywacji ReLU.\n",
    "\n",
    "Stwórzmy więc funkcję  **neuron_layer()**, której użyjemy do stworzenia jednej warstwy. \n",
    "Będziemy potrzebować parametrów do określenia \n",
    "  * wejść, \n",
    "  * liczby neuronów, \n",
    "  * funkcji aktywacji \n",
    "  * nazwy warstwy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b\n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przejdźmy przez ten kod linia po linii:\n",
    "\n",
    "  * Najpierw tworzymy zakres nazw używając nazwy warstwy: będzie on zawierał wszystkie węzły dla tej warstwy neuronu. \n",
    "  Jest to opcjonalne, ale wykres będzie wyglądał znacznie ładniej w TensorBoard.\n",
    "  * Następnie odczytujemy liczbę wejść (wymiar danych) poprzez sprawdzenie kształtu macierzy wejściowej (zerowy wymiar dotyczy instancji - ilościc danych).\n",
    "  * Następne trzy linie tworzą zmienną $W$, która będzie zawierała macierz wag. Będzie to tensor 2D zawierający wszystkie wagi połączeń między każdym wejściem a każdym neuronem - stąd jego kształt będzie (n_inputs, n_neurons). Zostanie zainicjalizowany losowo, przy użyciu rokładu **truncated normal**\n",
    "  * truncated normal to rozkład normalny z odchyleniem standardowym $2/\\sqrt{n_{inputs}}$. Wykorzystanie tego specyficznego odchylenia standardowego pomaga algorytmowi zbiegać znacznie szybciej. Ważne jest, aby inicjalizować losowe wagi dla wszystkich ukrytych warstw tak aby uniknąć symetrii, której algorytm Gradient Descent nie byłby w stanie zlikwidować.\n",
    "  * Następny wiersz tworzy zmienną dla bias, zainicjowaną na 0 (w tym przypadku nie ma problemu z symetrią).\n",
    "  * Następnie tworzymy podgraf aby obliczyć \n",
    "  $$\n",
    "  z = X · W + b.\n",
    "  $$\n",
    "  * Na koniec, jeśli parametr aktywacji jest ustawiony na \"relu\", kod zwraca $relu(z)$ lub w przeciwnym razie zwraca tylko $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tworzenia głębokiej sieci neuronowej\n",
    "\n",
    "Okej, więc teraz mamy funkcję tworzącą warstwy neuronów. Wykorzystajmy go do stworzenia głębokiej sieci neuronowej! \n",
    "\n",
    " * Pierwsza ukryta warstwa bierze $X$ jako swoje wejście.\n",
    " * Drugi pobiera dane wyjściowe pierwszej ukrytej warstwy jako dane wejściowe. \n",
    " * Na koniec, warstwa wyjściowa pobiera dane wyjściowe drugiej ukrytej warstwy jako dane wejściowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "    hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "    logits = neuron_layer(hidden2, n_outputs, \"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Zauważ, że ponownie użyliśmy zakresu nazw. \n",
    "  * Zauważ również, że **logits** są danymi wyjściowymi sieci neuronowej przed przejściem przez funkcję aktywacji softmax: \n",
    "    * ze względów optymalizacyjnych zajmiemy się później obliczaniem softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcja kosztu\n",
    "\n",
    "  * Teraz, gdy mamy gotowy model sieci neuronowej musimy zdefiniować funkcję kosztu, której użyjemy do jej uczenia. \n",
    "  * Podobnie jak w przypadku Regresji Softmax, użyjemy entropii krzyżowej. \n",
    "  * Entropia krzyżowa kara modele, który daje niskie prawdopodobieństwo dla klasy docelowej. \n",
    "  * TensorFlow udostępnia kilka funkcji do obliczania entropii krzyżowej. \n",
    "  * Użyjemy **sparse_softmax_cross_entropy_with_logits()**: \n",
    "    * oblicza entropię krzyżową na podstawie \"logits\" (tj. Wyjścia sieci przed przejściem przez funkcję aktywacji softmax) i oczekuje etykiet w postaci liczb całkowitych od 0 do liczby klas minus 1 (w naszym przypadku od 0 do 9). \n",
    "    * Zwróci ona tensor 1D zawierający entropię krzyżową dla każdej instancji. \n",
    "    \n",
    "    \n",
    "  * Możemy następnie użyć funkcji TensorFlow **redu_mean()**, aby obliczyć średnią wartość entropii krzyżowej we wszystkich instancjach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optymalizacja\n",
    "Mamy już model sieci neuronowej oraz funkcję kosztu. \n",
    "Teraz musimy zdefiniować GradientDescentOptimizer, który znajdzie parametry modelu które minimalizują funkcję kosztu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sposób oceny modelu\n",
    "  * Ostatnim ważnym krokiem jest określenie sposobu oceny modelu. \n",
    "  * Po prostu użyjemy **accuracy** jako naszej miary wydajności.\n",
    "    * Po pierwsze dla każdej instancji określ, czy prognoza sieci neuronowej jest prawidłowa, sprawdzając, czy najwyższy logit odpowiada klasie docelowej.\n",
    "    * Do tego możesz użyć funkcji **in_top_k()**. Zwraca ona tensor 1D wartości boolowskich, więc musimy zrzutować te wartości logiczne na wartości zmiennoprzecinkowe, a następnie obliczyć średnią. To da nam ogólną dokładność sieci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicjalizacja\n",
    "Musimy utworzyć węzeł, aby zainicjować wszystkie zmienne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uczenie\n",
    "\n",
    "## Najpierw załadujmy MNIST. \n",
    "\n",
    "Moglibyśmy używać Scikit-Learn do tego, tak jak to robiliśmy w poprzednich rozdziałach, ale TensorFlow oferuje własną funkcję, który pobiera dane, skaluje je (między 0 a 1), miesza je i zapewnia prostą funkcję ładowania jednego mini batcha. \n",
    "Więc użyjmy tego:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz definiujemy liczbę epok, które chcemy uruchomić, a także rozmiar batch-a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teraz możemy wytrenować model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%  0 Train accuracy: 0.9375 Test accuracy: 0.9065\n",
      "99%  1 Train accuracy: 0.96875 Test accuracy: 0.9266\n",
      "99%  2 Train accuracy: 0.9375 Test accuracy: 0.9358\n",
      "99%  3 Train accuracy: 0.953125 Test accuracy: 0.9435\n",
      "99%  4 Train accuracy: 0.953125 Test accuracy: 0.9458\n",
      "99%  5 Train accuracy: 0.890625 Test accuracy: 0.952\n",
      "99%  6 Train accuracy: 0.921875 Test accuracy: 0.9534\n",
      "99%  7 Train accuracy: 0.9375 Test accuracy: 0.9551\n",
      "99%  8 Train accuracy: 0.96875 Test accuracy: 0.9575\n",
      "99%  9 Train accuracy: 0.9375 Test accuracy: 0.957\n",
      "99%  10 Train accuracy: 0.984375 Test accuracy: 0.961\n",
      "99%  11 Train accuracy: 0.984375 Test accuracy: 0.9629\n",
      "99%  12 Train accuracy: 0.921875 Test accuracy: 0.9637\n",
      "99%  13 Train accuracy: 0.984375 Test accuracy: 0.9648\n",
      "99%  14 Train accuracy: 0.96875 Test accuracy: 0.965\n",
      "99%  15 Train accuracy: 0.96875 Test accuracy: 0.967\n",
      "99%  16 Train accuracy: 1.0 Test accuracy: 0.9677\n",
      "99%  17 Train accuracy: 0.953125 Test accuracy: 0.9679\n",
      "99%  18 Train accuracy: 0.96875 Test accuracy: 0.97\n",
      "99%  19 Train accuracy: 0.96875 Test accuracy: 0.9701\n",
      "99%  20 Train accuracy: 1.0 Test accuracy: 0.9699\n",
      "99%  21 Train accuracy: 0.96875 Test accuracy: 0.9709\n",
      "99%  22 Train accuracy: 1.0 Test accuracy: 0.972\n",
      "99%  23 Train accuracy: 1.0 Test accuracy: 0.972\n",
      "99%  24 Train accuracy: 1.0 Test accuracy: 0.9733\n",
      "99%  25 Train accuracy: 0.984375 Test accuracy: 0.9738\n",
      "99%  26 Train accuracy: 0.984375 Test accuracy: 0.9743\n",
      "99%  27 Train accuracy: 0.984375 Test accuracy: 0.9742\n",
      "99%  28 Train accuracy: 0.984375 Test accuracy: 0.9722\n",
      "99%  29 Train accuracy: 0.984375 Test accuracy: 0.9743\n",
      "99%  30 Train accuracy: 1.0 Test accuracy: 0.975\n",
      "99%  31 Train accuracy: 1.0 Test accuracy: 0.9754\n",
      "99%  32 Train accuracy: 1.0 Test accuracy: 0.9754\n",
      "99%  33 Train accuracy: 0.984375 Test accuracy: 0.9754\n",
      "99%  34 Train accuracy: 0.96875 Test accuracy: 0.9758\n",
      "99%  35 Train accuracy: 0.984375 Test accuracy: 0.9757\n",
      "99%  36 Train accuracy: 1.0 Test accuracy: 0.9757\n",
      "99%  37 Train accuracy: 1.0 Test accuracy: 0.9757\n",
      "99%  38 Train accuracy: 1.0 Test accuracy: 0.9767\n",
      "99%  39 Train accuracy: 1.0 Test accuracy: 0.9765\n",
      "99%  40 Train accuracy: 1.0 Test accuracy: 0.9761\n",
      "99%  41 Train accuracy: 1.0 Test accuracy: 0.9767\n",
      "99%  42 Train accuracy: 1.0 Test accuracy: 0.9763\n",
      "99%  43 Train accuracy: 0.984375 Test accuracy: 0.9763\n",
      "99%  44 Train accuracy: 1.0 Test accuracy: 0.9762\n",
      "99%  45 Train accuracy: 1.0 Test accuracy: 0.9776\n",
      "99%  46 Train accuracy: 0.984375 Test accuracy: 0.9768\n",
      "99%  47 Train accuracy: 1.0 Test accuracy: 0.9763\n",
      "99%  48 Train accuracy: 1.0 Test accuracy: 0.9771\n",
      "99%  49 Train accuracy: 1.0 Test accuracy: 0.9765\n",
      "99%  50 Train accuracy: 1.0 Test accuracy: 0.977\n",
      "99%  51 Train accuracy: 1.0 Test accuracy: 0.9769\n",
      "99%  52 Train accuracy: 1.0 Test accuracy: 0.9779\n",
      "99%  53 Train accuracy: 1.0 Test accuracy: 0.9775\n",
      "99%  54 Train accuracy: 1.0 Test accuracy: 0.9775\n",
      "99%  55 Train accuracy: 1.0 Test accuracy: 0.9781\n",
      "99%  56 Train accuracy: 1.0 Test accuracy: 0.9773\n",
      "99%  57 Train accuracy: 0.984375 Test accuracy: 0.978\n",
      "99%  58 Train accuracy: 1.0 Test accuracy: 0.9773\n",
      "99%  59 Train accuracy: 1.0 Test accuracy: 0.9785\n",
      "99%  60 Train accuracy: 1.0 Test accuracy: 0.9779\n",
      "99%  61 Train accuracy: 1.0 Test accuracy: 0.9772\n",
      "99%  62 Train accuracy: 1.0 Test accuracy: 0.978\n",
      "99%  63 Train accuracy: 1.0 Test accuracy: 0.978\n",
      "99%  64 Train accuracy: 1.0 Test accuracy: 0.9781\n",
      "99%  65 Train accuracy: 1.0 Test accuracy: 0.9781\n",
      "99%  66 Train accuracy: 1.0 Test accuracy: 0.9781\n",
      "99%  67 Train accuracy: 1.0 Test accuracy: 0.9783\n",
      "99%  68 Train accuracy: 1.0 Test accuracy: 0.9785\n",
      "99%  69 Train accuracy: 1.0 Test accuracy: 0.9786\n",
      "99%  70 Train accuracy: 1.0 Test accuracy: 0.9784\n",
      "99%  71 Train accuracy: 0.984375 Test accuracy: 0.9783\n",
      "99%  72 Train accuracy: 1.0 Test accuracy: 0.9788\n",
      "99%  73 Train accuracy: 1.0 Test accuracy: 0.9776\n",
      "99%  74 Train accuracy: 1.0 Test accuracy: 0.979\n",
      "99%  75 Train accuracy: 1.0 Test accuracy: 0.9791\n",
      "99%  76 Train accuracy: 1.0 Test accuracy: 0.9787\n",
      "99%  77 Train accuracy: 1.0 Test accuracy: 0.9784\n",
      "99%  78 Train accuracy: 1.0 Test accuracy: 0.9782\n",
      "99%  79 Train accuracy: 1.0 Test accuracy: 0.9779\n",
      "99%  80 Train accuracy: 1.0 Test accuracy: 0.979\n",
      "99%  81 Train accuracy: 1.0 Test accuracy: 0.9788\n",
      "99%  82 Train accuracy: 0.984375 Test accuracy: 0.9789\n",
      "99%  83 Train accuracy: 1.0 Test accuracy: 0.9786\n",
      "99%  84 Train accuracy: 1.0 Test accuracy: 0.9786\n",
      "99%  85 Train accuracy: 1.0 Test accuracy: 0.9793\n",
      "99%  86 Train accuracy: 1.0 Test accuracy: 0.9787\n",
      "99%  87 Train accuracy: 1.0 Test accuracy: 0.9789\n",
      "99%  88 Train accuracy: 1.0 Test accuracy: 0.9794\n",
      "99%  89 Train accuracy: 1.0 Test accuracy: 0.9792\n",
      "99%  90 Train accuracy: 1.0 Test accuracy: 0.9792\n",
      "99%  91 Train accuracy: 1.0 Test accuracy: 0.979\n",
      "99%  92 Train accuracy: 1.0 Test accuracy: 0.9787\n",
      "99%  93 Train accuracy: 1.0 Test accuracy: 0.9788\n",
      "99%  94 Train accuracy: 1.0 Test accuracy: 0.9792\n",
      "99%  95 Train accuracy: 1.0 Test accuracy: 0.9786\n",
      "99%  96 Train accuracy: 1.0 Test accuracy: 0.9787\n",
      "99%  97 Train accuracy: 1.0 Test accuracy: 0.9786\n",
      "99%  98 Train accuracy: 1.0 Test accuracy: 0.9788\n",
      "99%  99 Train accuracy: 1.0 Test accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(mnist.train.num_examples // batch_size):\n",
    "            print(\"\\r{}%\".format(100 * batch_index //  (mnist.train.num_examples // batch_size) ), end=\"\")\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(\" \", epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)    \n",
    "        \n",
    "    save_path = saver.save(sess, \"./MNIST_MLP/my_model_final.ckpt\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Ten kod otwiera sesję TensorFlow i uruchamia węzeł inicjujący, który inicjuje wszystkie zmienne. \n",
    "  * Następnie uruchamia główną pętlę treningową: w każdej epoce kod iteruje przez kilka batch-y odpowiadających rozmiarowi zestawu treningowego. \n",
    "  * Każda batch jest pobierana za pomocą metody **next_batch()**, a następnie kod po prostu uruchamia operację uczenia.   *   * \n",
    "  * Następnie, na końcu każdej epoki, kod ocenia model na ostatnim batch-u i na pełnym zestawie treningowym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Po nauczeniu sieci neuronowej można jej użyć do przewidywania. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./MNIST_MLP/my_model_final.ckpt\n",
      "True label  [6]\n",
      "Predicted label  [6]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./MNIST_MLP/my_model_final.ckpt\") \n",
    "    X_new_scaled, y_true = mnist.train.next_batch(1) # some new images (scaled from 0 to 1)\n",
    "    Z = logits.eval(feed_dict={X: X_new_scaled})\n",
    "    y_pred = np.argmax(Z, axis=1)\n",
    "print(\"True label \", y_true)\n",
    "print(\"Predicted label \", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uwaga! \n",
    "\n",
    "Istnieje również inna funkcja o nazwie **softmax_cross_entropy_with_logits()**, która pobiera etykiety w postaci **one-hot vectors ** (zamiast liczby od 0 do liczby klas minus 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
